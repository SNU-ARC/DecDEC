import argparse
from tqdm import tqdm
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
from collections import OrderedDict


argument_parser = argparse.ArgumentParser()
argument_parser.add_argument(
    "--original_model_path",
    type=str,
    required=True,
    help="Path to the original model before quantization."
)

argument_parser.add_argument(
    "--quantized_path",
    type=str,
    required=True,
    help="Path to the quantized model output generated by Any-Precision LLM, "
         "before it is packed into the final Any-Precision LLM format.",
)

argument_parser.add_argument(
    "--output_path",
    type=str,
    default=None,
    help="Path to save the dequantized model."
)


def main(args):
    # Check that the quantized path exists
    assert os.path.exists(args.quantized_path), f"Quantized path {args.quantized_path} does not exist."

    # Find the LUT directory
    lut_dirs = [
        d for d in os.listdir(args.quantized_path)
        if d.startswith("lut_") and d[4:].isdigit()
    ]
    assert len(lut_dirs) == 1, f"Expected exactly one LUT directory in {args.quantized_path}, found: {lut_dirs}"
    lut_root = os.path.join(args.quantized_path, lut_dirs[0])

    # Count layers
    num_layers = 0
    while os.path.exists(os.path.join(lut_root, f"l{num_layers}.pt")):
        num_layers += 1
    assert num_layers > 0, f"No layer files found in {lut_root}."

    print(f"Found {num_layers} layers in {lut_root}.")

    # Check weights directory
    weights_dir = os.path.join(args.quantized_path, "weights")
    assert os.path.exists(weights_dir), f"Weights directory {weights_dir} does not exist."
    for i in range(num_layers):
        path = os.path.join(weights_dir, f"l{i}.pt")
        assert os.path.exists(path), f"Missing weights file: {path}"

    # Load original model and tokenizer
    print(f"Loading original model from {args.original_model_path}...")
    config = AutoConfig.from_pretrained(args.original_model_path)
    model = AutoModelForCausalLM.from_pretrained(args.original_model_path, torch_dtype=torch.float16)
    tokenizer = AutoTokenizer.from_pretrained(args.original_model_path)

    # Prepare new weights
    print("Dequantizing model weights to create fake-quantized model...")
    new_state_dict = OrderedDict()
    for layer_idx in tqdm(range(num_layers), desc="Dequantizing layers"):
        lut_path = os.path.join(lut_root, f"l{layer_idx}.pt")
        weights_path = os.path.join(weights_dir, f"l{layer_idx}.pt")

        lut_dict = torch.load(lut_path, map_location='cpu', weights_only=False)
        weights_dict = torch.load(weights_path, map_location='cpu', weights_only=False)

        for module_name in lut_dict:
            lut_tensor = torch.tensor(lut_dict[module_name].squeeze(1), dtype=torch.float16)  # (d_out, 2**bitwidth)
            index_tensor = torch.tensor(weights_dict[module_name].squeeze(1), dtype=torch.int64)  # (d_out, d_in)

            dequantized = lut_tensor.gather(1, index_tensor)  # (d_out, d_in)

            # Note: if the model uses a different naming convention, you may need to adjust this
            state_dict_key = f"model.layers.{layer_idx}.{module_name}.weight"

            assert state_dict_key in model.state_dict(), f"Missing key {state_dict_key} in model state dict. Perhaps the parameter names are in a different format?"

            new_state_dict[state_dict_key] = dequantized

    # Update the model weights
    print("Updating model state dict with dequantized weights...")
    model_state_dict = model.state_dict()
    model_state_dict.update(new_state_dict)
    model.load_state_dict(model_state_dict)

    # Save the model and tokenizer
    print("Saving dequantized model...")
    assert not os.path.exists(args.output_path), f"Output path {args.output_path} already exists. Please choose a different path or remove the existing directory."
    os.makedirs(args.output_path, exist_ok=True)
    config.save_pretrained(args.output_path)
    model.save_pretrained(args.output_path)
    tokenizer.save_pretrained(args.output_path)

    print(f"Dequantized model saved to: {args.output_path}")


if __name__ == "__main__":
    args = argument_parser.parse_args()
    main(args)